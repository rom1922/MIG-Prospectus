{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dbf472b-dfb2-4f00-b7d3-797bc6b1f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14b5772d-0f7c-4965-8778-fa803fae1910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result_season(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fonction fournie par l'utilisateur pour la visualisation saisonnière.\n",
    "    y_true : Pandas Series avec DatetimeIndex\n",
    "    y_pred : Array numpy ou Pandas Series (valeurs prédites)\n",
    "    \"\"\"\n",
    "    dmap = {\n",
    "        12: 'DJF', 1: 'DJF', 2: 'DJF',\n",
    "        3: 'MAM', 4: 'MAM', 5: 'MAM',\n",
    "        6: 'JJA', 7: 'JJA', 8: 'JJA',\n",
    "        9: 'SON', 10: 'SON', 11: 'SON'\n",
    "    }\n",
    "    cmap = {\"DJF\": \"tab:blue\", \"MAM\": \"tab:green\",\n",
    "            \"JJA\": \"tab:red\", \"SON\": \"tab:orange\"}\n",
    "    \n",
    "    # Vérification et mappage des saisons\n",
    "    seasons = y_true.index.month.map(dmap)\n",
    "    colors = seasons.map(cmap)\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 4), constrained_layout=True)\n",
    "    gs = fig.add_gridspec(ncols=2, nrows=1, width_ratios=[2, 1])\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    # Plot 1 : Séries Temporelles\n",
    "    ax1.set_title(\"Capacity factor predictions\")\n",
    "    ax1.plot(y_true.index, y_true, color=\"tab:blue\", label=\"Actual\")\n",
    "    ax1.plot(y_true.index, y_pred, color=\"tab:red\", label=\"Predicted\")\n",
    "    ax1.set_xlim(y_true.index[0], y_true.index[-1])\n",
    "    ax1.legend(loc=\"lower right\", title=\"Capacity Factor\")\n",
    "\n",
    "    # Plot 2 : Scatter Plot (Actual vs Predicted)\n",
    "    ax2.set_title(\"Actual vs Predicted\")\n",
    "    ax2.set_xlabel(\"Actual\")\n",
    "    ax2.set_ylabel(\"Predicted\")\n",
    "    ax2.scatter(y_true, y_pred, c=colors, s=10)\n",
    "\n",
    "    # Diagonale idéale (y=x)\n",
    "    left, right = ax2.get_xlim()\n",
    "    bottom, top = ax2.get_ylim()\n",
    "    lb = min(left, bottom) - 0.01\n",
    "    ub = max(right, top) + 0.01\n",
    "    ax2.set_xlim(lb, ub)\n",
    "    ax2.set_ylim(lb, ub)\n",
    "    ax2.axline((lb, lb), (ub, ub), color=\"tab:red\")\n",
    "\n",
    "    # Légende des saisons\n",
    "    handles = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', label=s, \n",
    "                   markerfacecolor=cmap[s], markersize=6)\n",
    "        for s in [\"DJF\", \"MAM\", \"JJA\", \"SON\"]\n",
    "    ]\n",
    "    ax2.legend(handles=handles, title=\"Season\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f5aae97-9b3a-4362-9ffb-55a359efe071",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = pd.read_csv(\"data/CF_1d.csv\", index_col=\"Date\", parse_dates=[\"Date\"])\n",
    "ta = pd.read_csv(\"data/TA_1d.csv\", index_col=\"Date\", parse_dates=[\"Date\"])\n",
    "tp = pd.read_csv(\"data/TP_1d.csv\", index_col=\"Date\", parse_dates=[\"Date\"])\n",
    "\n",
    "cf = cf[[\"FR\"]]\n",
    "ta = ta.loc[:, ta.columns.str.startswith(\"FR\")]\n",
    "tp = tp.loc[:, tp.columns.str.startswith(\"FR\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7019b3d9-aa6c-4268-b1f8-5465eca5559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = pd.read_csv(\"data/CF_1d.csv\", index_col=\"Date\", parse_dates=[\"Date\"])\n",
    "ta = pd.read_csv(\"data/TA_1d.csv\", index_col=\"Date\", parse_dates=[\"Date\"])\n",
    "tp = pd.read_csv(\"data/TP_1d.csv\", index_col=\"Date\", parse_dates=[\"Date\"])\n",
    "\n",
    "cf = cf[[\"FR\"]]\n",
    "ta = ta.loc[:, ta.columns.str.startswith(\"FR\")]\n",
    "tp = tp.loc[:, tp.columns.str.startswith(\"FR\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8c7435a-b18a-4c6c-a2cc-1b761b4b6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([ta.mean(axis=1).rename(\"TA\"),\n",
    "                  tp.mean(axis=1).rename(\"TP\"),\n",
    "                  cf[\"FR\"].rename(\"CF\")], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fabba18-7044-4434-8f21-df85707c1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features & Target\n",
    "X = data.drop(columns=\"CF\")\n",
    "y = data[\"CF\"]\n",
    "\n",
    "#ajout d'une feunêtre\n",
    "X_days_TP = X[\"TP\"].rolling(window=7, min_periods=1).sum()\n",
    "X_days_TA = X[\"TA\"].rolling(window=60, min_periods=1).mean()\n",
    "X_days_TP = X_days_TP.reset_index(drop=True)\n",
    "X_days_TA = X_days_TA.reset_index(drop=True)\n",
    "\n",
    "#ajout des saisons\n",
    "#cos = [np.cos(2*np.pi*(t/365)) for t in range(3285)]\n",
    "#sin = [np.sin(2*np.pi*(t/365)) for t in range(3285)]\n",
    "#Saison = pd.DataFrame([[x,y] for x, y in zip(cos,sin)], columns = [5,6])\n",
    "\n",
    "#Création du tableau\n",
    "X2 = pd.concat([X_days_TP, X_days_TA], axis = 1)\n",
    "\n",
    "# Séparation des données d'entrainement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=365, shuffle=False)\n",
    "\n",
    "#Normalisation\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "#X_train_std = pd.concat([X_train_std, Saison[:][:2920]], axis=1)\n",
    "#X_test_std = pd.concat([X_test_std, Saison[:][2920:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd823a-5659-4860-9cc0-0f1f861d096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début de l'entraînement...\n",
      "Epoch [10/50], Train Loss: 0.0082, Test Loss: 0.0035\n",
      "Epoch [20/50], Train Loss: 0.0073, Test Loss: 0.0028\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Préparation des Données (Tenseurs & Séquences)\n",
    "# ==========================================\n",
    "torch.set_num_threads(11)\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Classe Dataset personnalisée pour transformer les données tabulaires\n",
    "    en séquences temporelles adaptées aux LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, sequence_length=30):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Entrée : séquence de taille (sequence_length, num_features)\n",
    "        # Cible : la valeur à t + sequence_length (ou t+1 selon l'horizon souhaité)\n",
    "        # Ici, on prédit la valeur cible correspondant au pas de temps suivant la séquence\n",
    "        return self.X[i:i+self.sequence_length], self.y[i+self.sequence_length]\n",
    "\n",
    "# Paramètres de dimensionnement basés sur votre pré-traitement\n",
    "# X_train_std contient 4 features : TP_rolling, TA_rolling, Cos_saison, Sin_saison\n",
    "SEQUENCE_LENGTH = 30  # Fenêtre d'observation (look-back window)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Création des Datasets et DataLoaders\n",
    "# Note : Assurez-vous que les index sont bien alignés (reset_index) avant cette étape\n",
    "train_dataset = TimeSeriesDataset(X_train_std, y_train, sequence_length=SEQUENCE_LENGTH)\n",
    "test_dataset = TimeSeriesDataset(X_test_std, y_test, sequence_length=SEQUENCE_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False) # Shuffle=False important pour séries temporelles si stateful, mais ici stateless est ok\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Architecture du Modèle LSTM\n",
    "# ==========================================\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Couche LSTM\n",
    "        # input_shape attendu : (batch_size, sequence_length, input_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Couche entièrement connectée pour la régression finale\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialisation des états cachés (h_0, c_0)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Propagation avant (Forward pass)\n",
    "        # out shape: (batch_size, seq_length, hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # On récupère seulement la sortie du dernier pas de temps de la séquence\n",
    "        # out[:, -1, :] shape: (batch_size, hidden_size)\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Prédiction finale\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Instanciation du modèle\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = X_train_std.shape[1] # Devrait être 4 (TP, TA, Cos, Sin)\n",
    "model = LSTMRegressor(input_size=input_dim, hidden_size=124, num_layers=3, output_size=1).to(device)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Boucle d'Entraînement et Optimisation\n",
    "# ==========================================\n",
    "\n",
    "criterion = nn.MSELoss() # Fonction de perte pour régression (Mean Squared Error)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(\"Début de l'entraînement...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batch_loss = []\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs.squeeze(), y_batch) # squeeze pour ajuster les dimensions\n",
    "        \n",
    "        # Backward pass et optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss.append(loss.item())\n",
    "    \n",
    "    train_epoch_loss = np.mean(batch_loss)\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    \n",
    "    # Évaluation (Validation)\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "            v_loss = criterion(outputs.squeeze(), y_val)\n",
    "            val_loss.append(v_loss.item())\n",
    "            \n",
    "    test_epoch_loss = np.mean(val_loss)\n",
    "    test_losses.append(test_epoch_loss)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_epoch_loss:.4f}, Test Loss: {test_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Entraînement terminé.\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Évaluation et Visualisation Avancée\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "\n",
    "# 1. Inférence (Récupération des prédictions brutes)\n",
    "with torch.no_grad():\n",
    "    for X_val, _ in test_loader: # On ignore y_val ici, on utilisera le pandas original\n",
    "        X_val = X_val.to(device)\n",
    "        outputs = model(X_val)\n",
    "        all_preds.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "y_pred_final = np.array(all_preds)\n",
    "\n",
    "# 2. Réalignement Temporel (CRITIQUE)\n",
    "# Le Dataset consomme les 'SEQUENCE_LENGTH' premiers jours pour créer la première fenêtre.\n",
    "# La première prédiction correspond donc à l'index : start_index + SEQUENCE_LENGTH\n",
    "# SEQUENCE_LENGTH a été défini à 14 dans le code précédent.\n",
    "\n",
    "trim_start = SEQUENCE_LENGTH \n",
    "# On récupère le sous-ensemble de y_test correspondant aux prédictions\n",
    "# y_test est la Pandas Series originale issue du train_test_split\n",
    "y_true_aligned = y_test.iloc[trim_start:]\n",
    "\n",
    "# Sécurité : Assurer que les longueurs correspondent exactement\n",
    "# (Le DataLoader peut parfois rejeter le dernier batch incomplet si drop_last=True, \n",
    "# bien que shuffle=False et drop_last=False soient par défaut ici)\n",
    "min_len = min(len(y_true_aligned), len(y_pred_final))\n",
    "y_true_aligned = y_true_aligned.iloc[:min_len]\n",
    "y_pred_final = y_pred_final[:min_len]\n",
    "\n",
    "print(f\"Dimensions alignées pour affichage : True {y_true_aligned.shape}, Pred {y_pred_final.shape}\")\n",
    "\n",
    "# 3. Appel de la fonction de visualisation\n",
    "r2 = r2_score(y_true_aligned, y_pred_final)\n",
    "mse = mean_squared_error(y_true_aligned, y_pred_final)\n",
    "\n",
    "print(f\"R2: {r2:.06f}\")\n",
    "print(f\"MSE: {mse:.06f}\")\n",
    "print()\n",
    "display_result_season(y_true_aligned, y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5bf02f-022e-4f7f-9f0d-88cee8e0b5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
