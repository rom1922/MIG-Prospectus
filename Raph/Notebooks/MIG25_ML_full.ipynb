{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e77c08",
   "metadata": {
    "editable": true,
    "id": "a5e77c08"
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "Ce notebook propose une première exploration des méthodes de modélisation appliquées à des données temporelles, afin d'illustrer comment le machine learning peut être utilisé pour estimer un facteur de charge à partir de séries chronologiques climatiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c77f657",
   "metadata": {
    "id": "5c77f657"
   },
   "source": [
    "## Généralités\n",
    "\n",
    "Les grandes étapes de la réalisation d'un modèle de machine learning :\n",
    "1. Préparation et Exploration\n",
    "  *  Nettoyage et préparation des données\n",
    "  *  Exploration et analyse des données (EDA)\n",
    "  *  Feature engineering\n",
    "2. Modélisation\n",
    "  *  Découpage du jeu de données\n",
    "  *  Choix du modèle\n",
    "  *  Entraînement et optimisation\n",
    "3. Évaluation et Interprétation\n",
    "  *  Évaluation du modèle\n",
    "  *  Interprétation et validation métier\n",
    "\n",
    "Ici nous omettons les étapes de collecte des données (étape 0) et de mise en production du modèle (étape 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb674d7",
   "metadata": {
    "id": "8eb674d7"
   },
   "source": [
    "**Contexte :**\n",
    "\n",
    "Nous disposons de données climatiques régionales de température et de précipitation pour la France continentale (21 régions NUTS2) de 2015 à 2023. Pour chaque année, nous disposons du **facteur de charge national** (NUTS0) des centrales hydroélectriques au fil de l’eau.\n",
    "\n",
    "**Objectifs :** explorer les données, construire des variables explicatives simples et tester plusieurs modèles de régression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b301756",
   "metadata": {
    "editable": true,
    "id": "9b301756"
   },
   "source": [
    "## 1. Préparation et Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de59683",
   "metadata": {
    "editable": true,
    "id": "4de59683"
   },
   "source": [
    "### Nettoyage et préparation des données\n",
    "\n",
    "Télécharger les données nécessaires pour l'analyse exploratoire. Les données sont décompressées dans le répertoire `data`:\n",
    "- `CF_1d.csv` : facteur de charge des centrales hydroélectriques au fil de l'eau au pas journalier de chaque pays européen,\n",
    "- `TA_1d.csv` : température moyenne de l'air au pas journalier de chaque région administrative de chaque pays européen,\n",
    "- `TP_1d.csv` : cumul des précipitations au pas journalier de chaque région administrative de chaque pays européen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d1224e",
   "metadata": {
    "id": "69d1224e"
   },
   "outputs": [],
   "source": [
    "!curl -sSL -q -o - \"https://cloud.minesparis.psl.eu/index.php/s/MGp21fRa8LEzO3f/download?path=%2F&files=mig25_data.tgz\" | tar -xzv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bfcd1c",
   "metadata": {
    "id": "36bfcd1c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823b72a7",
   "metadata": {
    "id": "823b72a7"
   },
   "source": [
    "1. Charger les données dans des dataframes nommés `cf`, `ta` et `tp` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c1897",
   "metadata": {
    "id": "025c1897"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c70dc",
   "metadata": {
    "editable": true,
    "id": "d20c70dc",
    "tags": [
     "prune-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cf = pd.read_csv(\"data/CF_1d.csv\", index_col=\"Date\", parse_dates=[\"Date\"])\n",
    "ta = pd.read_csv(\"data/TA_1d.csv\", index_col=\"Date\", parse_dates=[\"Date\"])\n",
    "tp = pd.read_csv(\"data/TP_1d.csv\", index_col=\"Date\", parse_dates=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951cb30",
   "metadata": {
    "editable": true,
    "id": "3951cb30"
   },
   "source": [
    "2. Extraire les données en rapport avec la France continentale (21 régions) pour chaque variable :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfafed2",
   "metadata": {
    "editable": true,
    "id": "9dfafed2"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f199e5",
   "metadata": {
    "editable": true,
    "id": "d8f199e5",
    "tags": [
     "prune-cell"
    ]
   },
   "outputs": [],
   "source": [
    "cf = cf[[\"FR\"]]\n",
    "ta = ta.loc[:, ta.columns.str.startswith(\"FR\")]\n",
    "tp = tp.loc[:, tp.columns.str.startswith(\"FR\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ffc978",
   "metadata": {
    "id": "85ffc978"
   },
   "source": [
    "### Exploration et analyse des données\n",
    "\n",
    "3. Afficher des informations de base sur les dataframes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84e83a",
   "metadata": {
    "id": "0a84e83a"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6902cb33",
   "metadata": {
    "editable": true,
    "id": "6902cb33",
    "tags": [
     "prune-cell"
    ]
   },
   "outputs": [],
   "source": [
    "cf.info()\n",
    "cf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a1998",
   "metadata": {
    "editable": true,
    "id": "814a1998",
    "tags": [
     "prune-cell"
    ]
   },
   "outputs": [],
   "source": [
    "ta.info()\n",
    "ta.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa0da0",
   "metadata": {
    "editable": true,
    "id": "12aa0da0",
    "tags": [
     "prune-cell"
    ]
   },
   "outputs": [],
   "source": [
    "tp.info()\n",
    "tp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f39560",
   "metadata": {
    "id": "66f39560"
   },
   "source": [
    "4. a) Visualiser les données disponibles pour une région :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa610fa",
   "metadata": {
    "id": "1fa610fa"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8da4b",
   "metadata": {
    "editable": true,
    "id": "53c8da4b"
   },
   "outputs": [],
   "source": [
    "region_names = {'FR10': 'Île de France', 'FRB0': 'Centre-Val de Loire',\n",
    "                'FRC1': 'Bourgogne', 'FRC2': 'Franche-Comté',\n",
    "                'FRD1': 'Basse-Normandie', 'FRD2': 'Haute-Normandie',\n",
    "                'FRE1': 'Nord-Pas-de-Calais', 'FRE2': 'Picardie',\n",
    "                'FRF1': 'Alsace', 'FRF2': 'Champagne-Ardenne',\n",
    "                'FRF3': 'Lorraine', 'FRG0': 'Pays de la Loire',\n",
    "                'FRH0': 'Bretagne', 'FRI1': 'Aquitaine',\n",
    "                'FRI2': 'Limousin', 'FRI3': 'Poitou-Charentes',\n",
    "                'FRJ1': 'Languedoc-Roussillon', 'FRJ2': 'Midi-Pyrénées',\n",
    "                'FRK1': 'Auvergne', 'FRK2': 'Rhône-Alpes',\n",
    "                'FRL0': 'Provence-Alpes-Côte d’Azur'}\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, sharex=True, figsize=(15, 9), constrained_layout=True)\n",
    "x_min, x_max = ta.index.min(), ta.index.max() + pd.DateOffset(days=1)\n",
    "\n",
    "region = \"FRL0\"  # PACA\n",
    "ax1.set_title(f\"{region_names[region]} ({region})\", fontsize=8)\n",
    "\n",
    "# Capacity Factor\n",
    "ax1.plot(cf[\"FR\"], lw=0.3, color=\"tab:green\")\n",
    "ax1.axvline(pd.Timestamp(2023, 1, 1), color=\"tab:red\", lw=0.5)\n",
    "ax1.tick_params(labelsize=6, labelbottom=True)\n",
    "ax1.set_ylabel(\"Facteur de charge\", fontsize=6)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Temperature\n",
    "ax2.plot(ta[region], lw=0.3, color=\"tab:orange\")\n",
    "ax2.axvline(pd.Timestamp(2023, 1, 1), color=\"tab:red\", lw=0.5)\n",
    "ax2.tick_params(labelsize=6, labelbottom=True)\n",
    "ax2.set_ylabel(\"Temp. (°K)\", fontsize=6)\n",
    "ax2.set_xlim(x_min, x_max)\n",
    "\n",
    "# Precipitation\n",
    "ax3.bar(tp.index, tp[region], lw=0.4, color='tab:blue', width=1)\n",
    "ax3.axvline(pd.Timestamp(2023, 1, 1), color=\"tab:red\", lw=0.5)\n",
    "ax3.tick_params(labelbottom=True, labelsize=6)\n",
    "ax3.set_ylabel(\"Précip. (m)\", fontsize=6)\n",
    "ax3.set_ylim(0, None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf7a5c7",
   "metadata": {
    "id": "2bf7a5c7"
   },
   "source": [
    "4. b) Comment pourriez-vous organiser ces données pour comparer les profils journaliers d'une année à l'autre ? Visualiser ces derniers sous forme de courbes et d'une heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71cdddd",
   "metadata": {
    "id": "c71cdddd"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428ee2f",
   "metadata": {
    "editable": true,
    "id": "9428ee2f"
   },
   "outputs": [],
   "source": [
    "piv_cf = cf.pivot_table(index=cf.index.dayofyear, columns=cf.index.year, values=\"FR\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(15, 9), sharex=True, constrained_layout=True)\n",
    "piv_cf.plot(ax=ax1, cmap=\"tab10\", lw=0.5, legend=False, title=\"Profils journaliers par année\")\n",
    "fig.legend()\n",
    "sns.heatmap(piv_cf.T, cmap=\"rainbow_r\", ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c2b9cf",
   "metadata": {
    "id": "c3c2b9cf"
   },
   "source": [
    "4. c) Comment pourriez-vous résumer statistiquement ces profils sur l’ensemble des années pour chaque jour (quantiles, moyenne, etc) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623cb7ff",
   "metadata": {
    "id": "623cb7ff"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b31ba",
   "metadata": {
    "editable": true,
    "id": "4e3b31ba",
    "tags": [
     "prune-cell"
    ]
   },
   "outputs": [],
   "source": [
    "stat_cf = pd.concat([piv_cf.quantile(np.linspace(0, 1, 5), axis=1).set_axis([\"min\", \"q1\", \"med\", \"q3\", \"max\"]).T,\n",
    "                     piv_cf.mean(axis=1).rename(\"mean\")], axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 9), constrained_layout=True)\n",
    "ax.set_title(\"Profils journaliers statistiques  \")\n",
    "stat_cf.plot(ax=ax, color=[\"black\", \"gray\", \"tab:red\", \"gray\", \"black\", \"tab:blue\"], lw=0.5)\n",
    "ax.set_xlim(0, 366)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0895d9cf",
   "metadata": {
    "id": "0895d9cf"
   },
   "source": [
    "### Feature engineering\n",
    "\n",
    "L'étape préliminaire dans le processus de développement d'un modèle de machine learning est de construire ses variables de décision pour qualifier ses observations. C'est une étape clé de l'ingénierie des données. Vous verrez que de mauvaises données (brutes, reconstruites ou composées) ne conduisent à aucun bon résultat.\n",
    "\n",
    "5. Construire un nouveau dataframe `data` de 3 colonnes : les températures moyennes, le cumul moyen des précipitations et le facteur de charge :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648bd217",
   "metadata": {
    "id": "648bd217"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ad98f",
   "metadata": {
    "editable": true,
    "id": "0c1ad98f"
   },
   "outputs": [],
   "source": [
    "data = pd.concat([ta.mean(axis=1).rename(\"TA\"),\n",
    "                  tp.mean(axis=1).rename(\"TP\"),\n",
    "                  cf[\"FR\"].rename(\"CF\")], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c328d3",
   "metadata": {
    "id": "c0c328d3"
   },
   "source": [
    "## Modélisation\n",
    "\n",
    "Avant d'attaquer réellement la modélisation, il nous reste une dernière étape de traitement de données. Il nous faut désormais séparer nos données en plusieurs jeux de données :\n",
    "\n",
    "- un jeu d'entraînement,\n",
    "- un jeu de validation,\n",
    "- un jeu de test.\n",
    "\n",
    "Selon les modèles d'apprentissage que nous sélectionnerons, nous aurons besoin de standardiser/normaliser nos valeurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92aa41f",
   "metadata": {
    "id": "e92aa41f"
   },
   "source": [
    "### Découpage du jeu de données\n",
    "\n",
    "6. a) Séparer les variables de décision et la cible en 2 variables `X` et `y`.  \n",
    "   b) Créer 2 jeux de données pour l'entrainement et le test  à l'aide de de la fonction [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).  \n",
    "   c) Standardiser les variables de décision avec [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9630ea",
   "metadata": {
    "editable": true,
    "id": "0c9630ea"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features & Target\n",
    "X = data.drop(columns=\"CF\")\n",
    "y = data[\"CF\"]\n",
    "\n",
    "# Séparation des données d'entrainement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=365, shuffle=False)\n",
    "\n",
    "# Normalisation\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "results = {\"Actual\": y_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2532ab",
   "metadata": {
    "id": "5f2532ab"
   },
   "source": [
    "### Choix du modèle\n",
    "\n",
    "Nous en avons fini avec les données, tout est prêt pour modéliser notre problème. Nous allons commencer avec des modèles simples de régression. Pour nos premiers pas, nous utiliserons les modèles suivants :\n",
    "\n",
    "* Régression linéaire : [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "* Régression linéaire avec pénalité L1 [`Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "* Régression linéaire avec pénalité L2 [`Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "* Arbre de décision : [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n",
    "\n",
    "Pour pousser plus loin, nous verrons également les modèles suivants :\n",
    "* Forêt aléatoire : [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "* Boosting de gradient : [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "\n",
    "*Les étapes d'évaluation et d'interprétation de la 3ème partie se feront en même temps que la modélisation et l'entrainement.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5a358",
   "metadata": {
    "id": "67d5a358"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def display_result(y_true, y_pred):\n",
    "    \"\"\"Affiche les résultats de prédiction / réels.\"\"\"\n",
    "    fig = plt.figure(figsize=(16, 4), constrained_layout=True)\n",
    "    gs = fig.add_gridspec(ncols=2, nrows=1, width_ratios=[2, 1])\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    # Plot 1\n",
    "    ax1.set_title(\"Capacity factor predictions\")\n",
    "    ax1.plot(y_true.index, y_true, color=\"tab:blue\", label=\"Actual\")\n",
    "    ax1.plot(y_true.index, y_pred, color=\"tab:red\", label=\"Predicted\")\n",
    "\n",
    "    ax1.set_xlim(y_true.index[0], y_true.index[-1])\n",
    "    ax1.legend(loc=\"lower right\", title=\"Capacity Factor\")\n",
    "\n",
    "    # Plot 2\n",
    "    ax2.set_title(\"Actual vs Predicted\")\n",
    "    ax2.set_xlabel(\"Actual\")\n",
    "    ax2.set_ylabel(\"Predicted\")\n",
    "    ax2.scatter(y_true, y_pred, color=\"tab:blue\", s=10)\n",
    "\n",
    "    left, right = ax2.get_xlim()\n",
    "    bottom, top = ax2.get_ylim()\n",
    "    lb = min(left, bottom) - 0.01\n",
    "    ub = max(right, top) + 0.01\n",
    "    ax2.set_ylim(lb, ub)\n",
    "    ax2.set_xlim(lb, ub)\n",
    "    ax2.axline((lb, lb), (ub, ub), color=\"tab:red\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d85b80",
   "metadata": {
    "id": "d2d85b80"
   },
   "source": [
    "#### 1. Régression linéaire\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\arg\\min_{\\beta}\n",
    "\\left(\n",
    "\\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\beta)^2\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Nous allons commencer par un modèle de régression linéaire `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33542c8",
   "metadata": {
    "id": "a33542c8"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()  # modèle de régression linéaire\n",
    "lr.fit(X_train, y_train)  # apprentissage supervisé\n",
    "\n",
    "y_pred = lr.predict(X_test)  # prédiction\n",
    "y_pred = pd.Series(y_pred, index=y_test.index)\n",
    "results[\"LinReg\"] = y_pred\n",
    "\n",
    "# Métriques\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Paramètres\n",
    "w_ta, w_tp = lr.coef_\n",
    "bias = lr.intercept_\n",
    "\n",
    "print(f\"R2: {r2:.06f}\")\n",
    "print(f\"MSE: {mse:.06f}\")\n",
    "print(f\"Weight[ta]: {w_ta:.6f}\")\n",
    "print(f\"Weight[tp]: {w_tp:.6f}\")\n",
    "print(f\"Bias: {bias:.6f}\")\n",
    "print()\n",
    "\n",
    "display_result(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2460da03",
   "metadata": {
    "id": "2460da03"
   },
   "source": [
    "#### 2. Régression Lasso (L1)\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\arg\\min_{\\beta}\n",
    "\\left(\n",
    "\\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\beta)^2\n",
    "\\;+\\;\n",
    "\\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Pour changer de modèle, c'est aussi simple que de changer son nom : de `LinearRegression` à `Lasso`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa77c31",
   "metadata": {
    "id": "ffa77c31"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()  # modèle de régression linéaire avec pénalité L1\n",
    "lasso.fit(X_train, y_train)  # apprentissage supervisé\n",
    "\n",
    "y_pred = lasso.predict(X_test)  # prédiction\n",
    "y_pred = pd.Series(y_pred, index=y_test.index)\n",
    "results[\"Lasso\"] = y_pred\n",
    "\n",
    "# Métriques\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Paramètres\n",
    "w_ta, w_tp = lasso.coef_\n",
    "bias = lasso.intercept_\n",
    "\n",
    "print(f\"R2: {r2:.06f}\")\n",
    "print(f\"MSE: {mse:.06f}\")\n",
    "print(f\"Weight[ta]: {w_ta:.6f}\")\n",
    "print(f\"Weight[tp]: {w_tp:.6f}\")\n",
    "print(f\"Bias: {bias:.6f}\")\n",
    "print()\n",
    "\n",
    "display_result(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d2e36",
   "metadata": {
    "id": "098d2e36"
   },
   "source": [
    "7. Observez les prédictions réalisez ? Pourquoi un tel comportement et d'où provient ce résultat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11e5b3",
   "metadata": {
    "id": "bb11e5b3"
   },
   "source": [
    "#### Régression Ridge (L2)\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\arg\\min_{\\beta}\n",
    "\\left(\n",
    "\\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\beta)^2\n",
    "\\;+\\;\n",
    "\\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Vous l'aurez compris pour faire un modèle `Ridge`, il suffit d'instancier le modèle du même nom. Ici nous allons observer 2 comportement différents selon les données passées à l'entrainement : données brutes ou données standardisées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01faf47a",
   "metadata": {
    "id": "01faf47a"
   },
   "source": [
    "**A. Sur données brutes :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422d062",
   "metadata": {
    "id": "5422d062"
   },
   "outputs": [],
   "source": [
    "# votre code ici\n",
    "\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0a7ea",
   "metadata": {
    "editable": true,
    "id": "7ac0a7ea"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge()  # modèle de régression linéaire avec pénalité L2\n",
    "ridge.fit(X_train, y_train)  # apprentissage supervisé\n",
    "\n",
    "y_pred = ridge.predict(X_test)  # prédiction\n",
    "y_pred = pd.Series(y_pred, index=y_test.index)\n",
    "results[\"Ridge\"] = y_pred\n",
    "\n",
    "# Métriques\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Paramètres\n",
    "w_ta, w_tp = ridge.coef_\n",
    "bias = ridge.intercept_\n",
    "\n",
    "print(f\"R2: {r2:.06f}\")\n",
    "print(f\"MSE: {mse:.06f}\")\n",
    "print(f\"Weight[ta]: {w_ta:.6f}\")\n",
    "print(f\"Weight[tp]: {w_tp:.6f}\")\n",
    "print(f\"Bias: {bias:.6f}\")\n",
    "print()\n",
    "\n",
    "display_result(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fdcfd7",
   "metadata": {
    "id": "60fdcfd7"
   },
   "source": [
    "**B. Sur données standardisées :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4474c87b",
   "metadata": {
    "id": "4474c87b"
   },
   "outputs": [],
   "source": [
    "# votre code ici\n",
    "\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e041594",
   "metadata": {
    "editable": true,
    "id": "3e041594"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge()  # modèle de régression linéaire avec pénalité L2\n",
    "ridge.fit(X_train_std, y_train)  # apprentissage supervisé\n",
    "\n",
    "y_pred = ridge.predict(X_test_std)  # prédiction\n",
    "y_pred = pd.Series(y_pred, index=y_test.index)\n",
    "results[\"Ridge(STD)\"] = y_pred\n",
    "\n",
    "# Métriques\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Paramètres\n",
    "w_ta, w_tp = ridge.coef_\n",
    "bias = ridge.intercept_\n",
    "\n",
    "print(f\"R2: {r2:.06f}\")\n",
    "print(f\"MSE: {mse:.06f}\")\n",
    "print(f\"Weight[ta]: {w_ta:.6f}\")\n",
    "print(f\"Weight[tp]: {w_tp:.6f}\")\n",
    "print(f\"Bias: {bias:.6f}\")\n",
    "print()\n",
    "\n",
    "display_result(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e49971",
   "metadata": {
    "id": "04e49971"
   },
   "source": [
    "8. Observez les prédictions réalisées. Que remarquez vous ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1509801f",
   "metadata": {
    "id": "1509801f"
   },
   "source": [
    "#### Arbre de décision\n",
    "\n",
    "Même si nous changeons de type de modèle, la méthodologie reste la même. Par contre, il est évident que les paramètres du modèle ne seront plus les mêmes (poids et biais pour la régression linéaire vs variables, seuils et valeurs de prédiction pour l'arbre de décision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1f6dd",
   "metadata": {
    "id": "4ae1f6dd"
   },
   "outputs": [],
   "source": [
    "# votre code ici\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb631a",
   "metadata": {
    "editable": true,
    "id": "5fcb631a"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor()  # modèle d'arbre de décision\n",
    "dt.fit(X_train, y_train)  # apprentissage supervisé\n",
    "\n",
    "y_pred = dt.predict(X_test)  # prédiction\n",
    "y_pred = pd.Series(y_pred, index=y_test.index)\n",
    "results[\"DT\"] = y_pred\n",
    "\n",
    "# Métriques\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"R2: {r2:.06f}\")\n",
    "print(f\"MSE: {mse:.06f}\")\n",
    "print()\n",
    "\n",
    "display_result(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ab82b",
   "metadata": {
    "id": "8e0ab82b"
   },
   "source": [
    "Nous pouvons visualiser sous forme de table les différents paramètres du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb09725",
   "metadata": {
    "id": "ceb09725"
   },
   "outputs": [],
   "source": [
    "dmap = dict(enumerate(X.columns)) | {-2: \"Leave\"}\n",
    "params = {\"Feature\": dt.tree_.feature,\n",
    "          \"Threshold\": dt.tree_.threshold,\n",
    "          \"Value\": dt.tree_.value.squeeze()}\n",
    "\n",
    "params = pd.DataFrame(params).replace({\"Feature\": dmap})\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e4ef2",
   "metadata": {
    "id": "9c0e4ef2"
   },
   "source": [
    "Il est même possible de visualiser facilement l'arbre de décision :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13fbc3",
   "metadata": {
    "editable": true,
    "id": "1b13fbc3"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 9))\n",
    "plot_tree(dt, feature_names=X.columns, filled=True, fontsize=10, max_depth=3, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe1695",
   "metadata": {
    "id": "b7fe1695"
   },
   "source": [
    "#### Recherche par grille\n",
    "\n",
    "Jusqu'à présent, nous avons utilisé nos 4 modèles sans configurer quoi ce soit :\n",
    "\n",
    "```\n",
    "lr = LinearRegression()\n",
    "lasso = Lasso()\n",
    "ridge = Ridge()\n",
    "dt = DecisionTreeRegressor()\n",
    "```\n",
    "\n",
    "Cela manque de souplesse n'est ce pas ? Comment régler correctement le coefficient de pénalité dans mes régressions ou définir la profondeur optimale de mon arbre de décision ? Il s'agit donc pour nous de configurer les meilleurs hyperparamètres du modèle afin de contrôler son apprentissage.\n",
    "\n",
    "Pour cela nous utiliserons une recherche par grille : [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "> **Note** : il ne faut pas confondre les paramètres d'une fonction (ou pluôt ses arguments) qui sont les hyperparamètres du modèle avec les paramètres du modèle qui sont les variables internes permettant de sortir une prédiction après apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606312e4",
   "metadata": {
    "id": "606312e4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": np.arange(1, 10),\n",
    "}\n",
    "reg = DecisionTreeRegressor(random_state=2024)  # modèle d'arbre de décision\n",
    "cv = GridSearchCV(reg, param_grid=params)  # recherche par grille\n",
    "cv.fit(X_train, y_train)  # apprentissage supervisé\n",
    "\n",
    "y_pred = cv.predict(X_test)  # prédiction\n",
    "y_pred = pd.Series(y_pred, index=y_test.index)\n",
    "results[\"DTCV\"] = y_pred\n",
    "\n",
    "# Métriques\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"R2: {r2:.06f}\")\n",
    "print(f\"MSE: {mse:.06f}\")\n",
    "print()\n",
    "\n",
    "display_result(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cd69f",
   "metadata": {
    "id": "034cd69f"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9facf187",
   "metadata": {
    "id": "9facf187"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 9))\n",
    "plot_tree(cv.best_estimator_, feature_names=X.columns, filled=True, fontsize=10, max_depth=3, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30b44a6",
   "metadata": {
    "id": "b30b44a6"
   },
   "source": [
    "### Evaluation et Interprétation\n",
    "\n",
    "Nous avons constaté que nos modèles ne sont pas bons mais nous n'avons pas pu les visualiser simultanément sur un même graphique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f94cc",
   "metadata": {
    "id": "4c9f94cc"
   },
   "outputs": [],
   "source": [
    "dfr = pd.DataFrame(results)\n",
    "dfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14273049",
   "metadata": {
    "id": "14273049"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=3, figsize=(15, 9), sharex=True, sharey=True, constrained_layout=True)\n",
    "\n",
    "for ax, col in zip(axs.flatten(), dfr.columns[1:]):\n",
    "  dfr[col].plot(ax=ax, lw=0.8, color=\"tab:red\", title=col)\n",
    "  dfr[\"Actual\"].plot(ax=ax, lw=0.8, color=\"tab:blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b20c34a",
   "metadata": {
    "id": "4b20c34a"
   },
   "source": [
    "# Et maintenant ?\n",
    "\n",
    "Nous avons vu que nos simples variables ne sont pas suffisantes pour réaliser un modèle performant. Toutefois cela nous a permis de développer rapidement un premier modèle d'apprentissage automatique.\n",
    "\n",
    "Désormais, il va nous falloir créer des variables explicatives plus en adéquation avec le problème que nous tentons de modéliser.\n",
    "\n",
    "Sans être hydrologue ou météorologue, il est nécessaire de comprendre un minimum les phénomènes physiques liés au cycle de l'eau :\n",
    "\n",
    "![Cycle de l'eau](https://geotechniquehse.com/wp-content/uploads/2024/10/hydrogeologie-cycle-de-leau.png)\n",
    "\n",
    "9. **Que proposeriez vous comme nouvelles variables explicatives ?**\n",
    "\n",
    "**La réponse a cette question passe par l'étude de la corrélation spatiale et temporelle qui lie les variables climatiques au facteur de charge.**\n",
    "\n",
    "Quand vous aurez des variables en adéquation avec votre problème, vous pourrez utiliser des modèles plus performants comme les forêts aléatoire et le boosting de gradient, voire des réseaux de neurones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d307d3",
   "metadata": {
    "id": "27d307d3"
   },
   "source": [
    "# Informations (très) utiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb9f8f",
   "metadata": {
    "id": "e3fb9f8f"
   },
   "source": [
    "## Méthodes d'encodage des données pour l'apprentissage\n",
    "\n",
    "Tout au long du processus de création des variables, il arrivera que nous devions les mettre sous une autre forme. Voici quelques une des principales transformations :\n",
    "\n",
    "1. [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)  \n",
    "   **Objectif :** Transformer des variables catégorielles en une forme que les modèles peuvent comprendre en créant une colonne binaire pour chaque catégorie.  \n",
    "   **A utiliser :**\n",
    "   - pour les variables **nominales** (sans ordre ou relation entre les catégories).\n",
    "   - lorsqu'il y a des catégories discrètes et qu'il est nécessaire de les traiter indépendamment.\n",
    "\n",
    "2. [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)  \n",
    "   **Objectif :** Standardiser les données numériques de manière à ce qu’elles soient centrées autour de leur moyenne et comparables par leur écart-type. Cela permet de traiter des variables avec différentes échelles.  \n",
    "   **A utiliser :**\n",
    "   - pour les modèles sensibles à l'échelle des données, tels que les régressions linéaires, les SVM, ou les réseaux de neurones.\n",
    "   - lorsque les données ont des unités différentes ou des amplitudes différentes.\n",
    "\n",
    "3. [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)  \n",
    "   **Objectif :** Mettre à l'échelle les données entre une plage spécifiée, généralement entre 0 et 1.  \n",
    "   **A utiliser :**\n",
    "   - lorsqu'il est nécessaire que les données soient dans un intervalle spécifique, surtout pour les modèles sensibles à l'échelle (comme les réseaux de neurones, où l'activation se fait souvent sur des valeurs entre 0 et 1).\n",
    "   - lorsque les données doivent être dans un certain intervalle.\n",
    "\n",
    "4. Cyclical Features Encoding  \n",
    "   **Objectif :** Capturer la relation cyclique des données saisonnières en les transformant sur un cercle unitaire avec les fonctions trigonométriques `sin` et `cos`.  \n",
    "   **A utiliser :**\n",
    "     - pour des données cycliques.\n",
    "     - lorsqu'il est nécessaire de préserver l'ordre temporel et la continuité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f2b85",
   "metadata": {
    "id": "037f2b85"
   },
   "source": [
    "## Méthodes de découpage des données pour la validation croisée\n",
    "\n",
    "La création de jeux de données de séries temporelles dans le cadre de prévision se fait rarement de manière aléatoire. Cela peut entraîner des problèmes de généralisation et ne représente pas le cas d'usage principal.\n",
    "\n",
    "1. **[`TimeSeriesSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) : Découpage temporel séquentiel**  \n",
    "   - Convient pour les séries temporelles où l'ordre chronologique est crucial.  \n",
    "   - Les données sont découpées de manière progressive : chaque split utilise une portion plus grande des données passées pour l'entraînement, et les données futures pour le test.  \n",
    "   - Les indices sont respectés pour ne pas mélanger les informations futures dans l'entraînement.  \n",
    "   - Exemple :\n",
    "     - Split 1 : Train = [2015], Test = [2016]  \n",
    "     - Split 2 : Train = [2015, 2016], Test = [2017]  \n",
    "     - Split 3 : Train = [2015, 2016, 2017], Test = [2018].\n",
    "\n",
    "2. **[`GroupKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html) : Découpage par groupes (par exemple, années)**  \n",
    "   - Permet de s'assurer que les groupes (comme les années ou d'autres identifiants logiques) ne sont jamais mélangés entre l'entraînement et le test.  \n",
    "   - Chaque split utilise des groupes différents pour l'entraînement et le test.  \n",
    "   - Utile lorsque les données doivent rester groupées par identifiant logique.  \n",
    "   - Exemple :\n",
    "     - Split 1 : Train = [2016, 2017, 2018], Test = [2015]  \n",
    "     - Split 2 : Train = [2015, 2017, 2018], Test = [2016]  \n",
    "     - Split 3 : Train = [2015, 2016, 2018], Test = [2017].\n",
    "     - Split 4 : Train = [2015, 2016, 2017], Test = [2018].\n",
    "\n",
    "> **Note :** Ces deux méthodes peuvent être directement utilisées comme paramètre de [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) pour optimiser les hyperparamètres tout en respectant les spécificités des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65fcad7",
   "metadata": {
    "id": "b65fcad7"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_comp = ta1.shape[1]\n",
    "comp = np.arange(1, n_comp + 1)\n",
    "\n",
    "pca = PCA(n_components=n_comp)\n",
    "pca.fit(ta)\n",
    "\n",
    "var_ratio = pca.explained_variance_ratio_\n",
    "cum_var = var_ratio.cumsum()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(18, 6))\n",
    "\n",
    "ax1.plot(comp, var_ratio, marker=\"o\")\n",
    "ax1.set_xlabel(\"Composante principale\")\n",
    "ax1.set_ylabel(\"Part de variance expliquée\")\n",
    "ax1.xaxis.set_major_locator(mpl.ticker.FixedLocator(comp))\n",
    "\n",
    "ax2.plot(comp, cum_var, marker=\"o\")\n",
    "ax2.set_xlabel(\"Composante principale\")\n",
    "ax2.set_ylabel(\"Variance expliquée cumulée\")\n",
    "\n",
    "fig.suptitle(\"Analyse en Composante Principale (ACP)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ab915",
   "metadata": {
    "id": "4a2ab915"
   },
   "outputs": [],
   "source": [
    "cols = [\"PC1\", \"PC2\"]\n",
    "k = 2\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "Z = pca2.fit_transform(ta)\n",
    "Z = pd.DataFrame(Z, columns=cols)\n",
    "\n",
    "corr = pca2.components_.T * np.sqrt(pca2.explained_variance_)  # poids * variance de la CP\n",
    "corr_df = pd.DataFrame(corr, index=list(ta.columns), columns=cols)\n",
    "\n",
    "ctr = (corr**2) / (corr**2).sum(axis=0, keepdims=True)  # calcul de la contribution\n",
    "ctr_df = pd.DataFrame(ctr, index=list(ta.columns), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664bbb6",
   "metadata": {
    "id": "4664bbb6"
   },
   "outputs": [],
   "source": [
    "dmap = {12: 'DJF', 1: 'DJF', 2: 'DJF',\n",
    "        3: 'MAM', 4: 'MAM', 5: 'MAM',:\n",
    "        6: 'JJA', 7: 'JJA', 8: 'JJA',\n",
    "        9: 'SON', 10: 'SON', 11: 'SON'}\n",
    "cmap = {\"DJF\": \"tab:blue\", \"MAM\": \"tab:green\",\n",
    "        \"JJA\": \"tab:red\", \"SON\": \"tab:orange\"}\n",
    "\n",
    "seas = X.index.month.map(dmap)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.scatterplot(data=X, x=\"TP\", y=\"TA\", hue=seas, palette=cmap, marker='^', ax=ax)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
